import argparse
import os
from typing import List

import torch
import torch.distributed as dist
from omegaconf import OmegaConf
from tqdm import tqdm
from torch.utils.data import DataLoader, SequentialSampler
from torch.utils.data.distributed import DistributedSampler
from torchvision.io import write_video
from torchvision import transforms  # noqa: F401  # ä¿ç•™ä¸åŸè„šæœ¬ä¸€è‡´çš„å¯¼å…¥
from einops import rearrange

from utils.misc import set_seed
from utils.distributed import barrier  # ä½¿ç”¨é¡¹ç›®ä¸­çš„barrierå‡½æ•°
from utils.memory import gpu, get_cuda_free_memory_gb, DynamicSwapInstaller

from pipeline.interactive_causal_inference_v2 import InteractiveCausalInferencePipeline
from utils.dataset import MultiTextDataset
import gradio

import time
import tempfile
import imageio.v3 as iio  # pip install imageio
import cv2
import base64
import numpy as np
from gradio.themes.utils.colors import Color as ColorTemplate
from nvidia import Nvidia
from gradio.themes.utils import colors, fonts, sizes

# ----------------------------- Argument è§£æ -----------------------------
parser = argparse.ArgumentParser("Prompt-multiple-switch inference")
parser.add_argument("--config_path", type=str, help="Path to the config file")
args = parser.parse_args()

config = OmegaConf.load(args.config_path)

# ----------------------------- Distributed è®¾ç½® -----------------------------
if "LOCAL_RANK" in os.environ:
    # è®¾ç½®NCCLç¯å¢ƒå˜é‡ä»¥é¿å…hang
    os.environ["NCCL_CROSS_NIC"] = "1"
    os.environ["NCCL_DEBUG"] = os.environ.get("NCCL_DEBUG", "INFO")
    os.environ["NCCL_TIMEOUT"] = os.environ.get("NCCL_TIMEOUT", "1800")

    local_rank = int(os.environ["LOCAL_RANK"])
    world_size = int(os.environ.get("WORLD_SIZE", "1"))
    rank = int(os.environ.get("RANK", str(local_rank)))

    # å…ˆè®¾ç½®è®¾å¤‡
    torch.cuda.set_device(local_rank)
    device = torch.device(f"cuda:{local_rank}")

    # åˆå§‹åŒ–process groupæ—¶æŒ‡å®šbackendå’Œtimeout
    if not dist.is_initialized():
        dist.init_process_group(
            backend="nccl",
            rank=rank,
            world_size=world_size,
            timeout=torch.distributed.constants.default_pg_timeout
        )

    set_seed(config.seed + local_rank)
    print(f"[Rank {rank}] Initialized distributed processing on device {device}")
else:
    local_rank = 0
    rank = 0
    device = torch.device("cuda")
    set_seed(config.seed)
    print(f"Single GPU mode on device {device}")

low_memory = get_cuda_free_memory_gb(device) < 40

torch.set_grad_enabled(False)

pipeline = InteractiveCausalInferencePipeline(config, device=device)

if config.generator_ckpt:
    state_dict = torch.load(config.generator_ckpt, map_location="cpu")
    raw_gen_state_dict = state_dict["generator_ema" if config.use_ema else "generator"]

    if config.use_ema:
        def _clean_key(name: str) -> str:
            return name.replace("_fsdp_wrapped_module.", "")


        cleaned_state_dict = {_clean_key(k): v for k, v in raw_gen_state_dict.items()}
        missing, unexpected = pipeline.generator.load_state_dict(
            cleaned_state_dict, strict=False
        )
        if local_rank == 0:
            if missing:
                print(f"[Warning] {len(missing)} parameters missing: {missing[:8]} ...")
            if unexpected:
                print(f"[Warning] {len(unexpected)} unexpected params: {unexpected[:8]} ...")
    else:
        pipeline.generator.load_state_dict(raw_gen_state_dict)

# --------------------------- LoRA support (optional) ---------------------------
# åº”ç”¨ä¸åŠ è½½ LoRAï¼ˆä»…åœ¨æä¾› config.adapter æ—¶å¯ç”¨ï¼‰ã€‚
try:
    from utils.lora_utils import configure_lora_for_model
    import peft
except Exception as e:
    configure_lora_for_model = None
    peft = None
    if getattr(config, "adapter", None):
        if local_rank == 0:
            print(f"[Warning] LoRA requested but dependencies unavailable: {e}")

pipeline.is_lora_enabled = False
if getattr(config, "adapter", None) and configure_lora_for_model is not None:
    if local_rank == 0:
        print(f"LoRA enabled with config: {config.adapter}")
        print("Applying LoRA to generator (inference)...")
    # åœ¨åŠ è½½åŸºç¡€æƒé‡åï¼Œå¯¹ generator çš„ transformer æ¨¡å‹åº”ç”¨ LoRA åŒ…è£…
    pipeline.generator.model = configure_lora_for_model(
        pipeline.generator.model,
        model_name="generator",
        lora_config=config.adapter,
        is_main_process=(local_rank == 0),
    )

    # åŠ è½½ LoRA æƒé‡ï¼ˆå¦‚æœæä¾›äº† lora_ckptï¼‰
    lora_ckpt_path = getattr(config, "lora_ckpt", None)
    if lora_ckpt_path:
        if local_rank == 0:
            print(f"Loading LoRA checkpoint from {lora_ckpt_path}")
        lora_checkpoint = torch.load(lora_ckpt_path, map_location="cpu")
        # å…¼å®¹åŒ…å« `generator_lora` é”®æˆ–ç›´æ¥æ˜¯ LoRA state dict ä¸¤ç§æ ¼å¼
        if isinstance(lora_checkpoint, dict) and "generator_lora" in lora_checkpoint:
            peft.set_peft_model_state_dict(pipeline.generator.model, lora_checkpoint["generator_lora"])  # type: ignore
        else:
            peft.set_peft_model_state_dict(pipeline.generator.model, lora_checkpoint)  # type: ignore
        if local_rank == 0:
            print("LoRA weights loaded for generator")
    else:
        if local_rank == 0:
            print("No LoRA checkpoint specified; using base weights with LoRA adapters initialized")

    pipeline.is_lora_enabled = True

# Move pipeline to appropriate dtype and device
pipeline = pipeline.to(dtype=torch.bfloat16)
if low_memory:
    DynamicSwapInstaller.install_model(pipeline.text_encoder, device=device)
pipeline.generator.to(device=device)
pipeline.vae.to(device=device)

# ----------------------------- æ„å»ºæ•°æ®é›† -----------------------------
# è§£æ switch_frame_indices
switch_frame_indices: List[int] = [int(x) for x in config.switch_frame_indices.split(",") if x.strip()]

# åˆ›å»ºè¾“å‡ºç›®å½•
if local_rank == 0:
    os.makedirs(config.output_folder, exist_ok=True)

if dist.is_initialized():
    dist.barrier()

global_prompts = []


@torch.no_grad()
def synthesize_video_from_prompt(prompt):
    sampled_noise = torch.randn(
        [
            config.num_samples,
            42,
            16,
            60,
            104,
        ],
        device=device,
        dtype=torch.bfloat16,
    )
    global_prompts.append(prompt)
    switch_frame_indices = [int(i) for i in torch.arange(1, len(global_prompts)) * 40]

    video = pipeline.inference(
        noise=sampled_noise,
        text_prompt=prompt,
        switch_frame_indices=[switch_frame_indices[-1]] if len(global_prompts) > 1 else [],
        return_latents=False,
    )

    current_video = rearrange(video, "b t c h w -> b t h w c").cpu() * 255.0

    if dist.is_initialized():
        rank = dist.get_rank()
    else:
        rank = 0

    # Determine model type for filename
    if hasattr(pipeline, 'is_lora_enabled') and pipeline.is_lora_enabled:
        model_type = "lora"
    elif getattr(config, 'use_ema', False):
        model_type = "ema"
    else:
        model_type = "regular"

    for seed_idx in range(config.num_samples):
        if config.save_with_index:
            output_path = os.path.join(config.output_folder, f"rank{rank}-{seed_idx}_{model_type}.mp4")
        else:
            # å–ç¬¬ä¸€æ®µ prompt ä½œä¸ºæ–‡ä»¶åå‰ç¼€ï¼Œé¿å…è¿‡é•¿
            short_name = prompts_list[0][0][:100].replace("/", "_")
            output_path = os.path.join(config.output_folder, f"rank{rank}-{short_name}-{seed_idx}_{model_type}.mp4")
        write_video(output_path, current_video[seed_idx].to(torch.uint8), fps=16)

    return output_path
    # if config.inference_iter != -1 and i >= config.inference_iter:
    #     break


@torch.no_grad()
def concat_history_videos(history_paths):
    """å°†åŒä¸€ä¼šè¯ä¸­çš„å†å²è§†é¢‘æŒ‰é¡ºåºæ‹¼æ¥åœ¨ä¸€èµ·ï¼Œæœ€å¤šæ‹¼æ¥ 6 æ®µã€‚

    å‚æ•°
    ----
    history_paths : List[str]
        å½“å‰ä¼šè¯å†…å·²ç»ç”Ÿæˆçš„æ‰€æœ‰æˆç‰‡è·¯å¾„ï¼ˆæŒ‰æ—¶é—´é¡ºåºä¿å­˜ï¼‰ã€‚

    è¿”å›
    ----
    str æˆ– None
        æ‹¼æ¥åçš„è§†é¢‘è·¯å¾„ï¼›è‹¥æ²¡æœ‰å¯ç”¨è§†é¢‘åˆ™è¿”å› Noneã€‚
    """
    import os
    import tempfile
    import numpy as np
    import imageio.v3 as iio

    if not history_paths:
        # æ²¡æœ‰å†å²è§†é¢‘ï¼Œå‰ç«¯ File ç»„ä»¶ä¼šæ˜¾ç¤ºä¸ºç©º
        return None

    # åªå–æœ€è¿‘çš„ 6 æ®µï¼Œè¶…è¿‡ 6 æ®µè§†ä¸ºâ€œé‡æ–°å¼€å§‹æ‹¼æ¥â€
    selected_paths = list(history_paths)[-6:]

    all_frames = []
    for p in selected_paths:
        if not os.path.exists(p):
            continue
        # è¯»å–æ•´æ®µè§†é¢‘çš„æ‰€æœ‰å¸§ï¼Œä¿æŒåŸæœ‰åˆ†è¾¨ç‡
        vid = iio.imread(p)   # å½¢çŠ¶ä¸€èˆ¬ä¸º (T, H, W, C)
        if vid.ndim == 3:
            # ä¸‡ä¸€ç¼ºé€šé“ç»´ï¼Œç®€å•æ‰©å±•
            vid = vid[..., None]
        all_frames.append(vid)

    if not all_frames:
        return None

    concat_frames = np.concatenate(all_frames, axis=0)

    os.makedirs(config.output_folder, exist_ok=True)
    with tempfile.NamedTemporaryFile(
        prefix="longlive_concat_",
        suffix=".mp4",
        delete=False,
        dir=config.output_folder,
    ) as tmpf:
        out_path = tmpf.name

    # ä¸å•æ®µè§†é¢‘ä¸€è‡´ï¼Œä½¿ç”¨ 16fps
    iio.imwrite(out_path, concat_frames, fps=16)

    return out_path


@torch.no_grad()
def on_generate_stream(prompt, seed, history_paths):
    import time, tempfile, os
    import numpy as np
    import imageio.v3 as iio
    from einops import rearrange

    # åˆå§‹åŒ–æˆ–æ¢å¤å†å²åˆ—è¡¨ï¼ˆå½“å‰ä¼šè¯å†…çš„æ‰€æœ‰æˆç‰‡è·¯å¾„ï¼‰
    if history_paths is None:
        history_paths = []
    # ç®€å•ä¿è¯æ˜¯ list ç±»å‹
    history_paths = list(history_paths)

    torch.manual_seed(int(seed))
    prompt = (prompt or "").strip()

    # é‡‡æ ·åˆå™ªï¼ˆä¿æŒä½ çš„å½¢çŠ¶ï¼‰
    sampled_noise = torch.randn(
        [config.num_samples, 42, 16, 60, 104],
        device=device,
        dtype=torch.bfloat16,
    )

    # æˆç‰‡è¾“å‡ºï¼ˆä¸´æ—¶æ–‡ä»¶ï¼Œé¿å…å¹¶å‘è¦†ç›–ï¼‰
    os.makedirs(config.output_folder, exist_ok=True)
    with tempfile.NamedTemporaryFile(
        prefix="longlive_",
        suffix=".mp4",
        delete=False,
        dir=config.output_folder,
    ) as tmpf:
        out_path = tmpf.name

    # è·å–é€å¸§ç”Ÿæˆå™¨ï¼ˆé€å¸§äº§å‡º HWC uint8ï¼‰
    frame_iter = pipeline.inference(
        noise=sampled_noise,
        text_prompt=prompt,
        switch_frame_indices=[],
        return_latents=False,
    )

    frames = []
    t0 = time.time()

    for frame in frame_iter:
        # è‹¥æ˜¯ torch.Tensorï¼Œè½¬æˆ uint8 HWC numpy
        if isinstance(frame, torch.Tensor):
            t = frame.detach().to("cpu")
            if t.dtype.is_floating_point:
                t = (t * 255.0).clamp(0, 255).to(torch.uint8)
            if t.ndim == 3 and t.shape[0] in (1, 3):  # (C,H,W)->(H,W,C)
                t = rearrange(t, "c h w -> h w c")
            frame = t.numpy()
        elif isinstance(frame, np.ndarray) and frame.dtype != np.uint8:
            frame = np.clip(frame, 0, 255).astype(np.uint8)

        frames.append(frame)

    # å°è£… mp4
    iio.imwrite(out_path, frames, fps=16)

    dt = time.time() - t0  # è¿™é‡Œç›®å‰åªæ˜¯ä¿ç•™æ—¶é•¿ä¿¡æ¯ï¼Œå¦‚æœ‰éœ€è¦å¯ç”¨äºæ—¥å¿—

    # æ›´æ–°å†å²ï¼šè¿½åŠ å½“å‰æˆç‰‡ï¼Œå¹¶é™åˆ¶é•¿åº¦é¿å…æ— é™å¢é•¿ï¼ˆè¿™é‡Œä¿ç•™æœ€è¿‘ 100 æ®µï¼‰
    history_paths.append(out_path)
    if len(history_paths) > 100:
        history_paths = history_paths[-100:]

    # æœ€ç»ˆè¾“å‡ºæˆç‰‡è·¯å¾„ + å†å²åˆ—è¡¨ï¼ˆå‰ç«¯è§†é¢‘ç»„ä»¶ä¼šè‡ªåŠ¨åŠ è½½ï¼‰
    return out_path, history_paths

nv_green = ColorTemplate(
    name="nv_green",
    c50="#f2f9f3",  # Light shade of nv green
    c100="#e5f3eb",  # Lighter shade of nv green
    c200="#c7e6d9",  # Mid-tone nv green
    c300="#76b900",  # nv green
    c400="#48873a",  # Darker shade of nv green
    c500="#30692c",  # Dark shade of nv green
    c600="#245121",  # Very dark shade of nv green
    c700="#1b3a17",  # Very dark shade of nv green
    c800="#12280e",  # Very dark shade of nv green
    c900="#0a1909",  # Very dark shade of nv green
    c950="#081407",  # Very dark shade of nv green
)

# åˆ›å»º Gradio ç•Œé¢
with gradio.Blocks(
    theme=Nvidia(
        primary_hue=nv_green,
        secondary_hue=nv_green,
        neutral_hue=colors.gray,
    ),
    title="LongLive Playground",

    js="async () => {" + open("./app_script.js", "r", encoding="utf-8").read() + "}",
    css="./app_style.css",) as demo:
    # é¡¶éƒ¨æ ‡é¢˜
    with gradio.Column(scale=1, min_width=400):
        #gradio.Image("images/logo.png", show_label=False, elem_id="logo")
        gradio.Markdown(
            """
            <h1 style="text-align: center; font-size: 35px; margin-top: 1px;">
                ğŸ¬ LongLive: Real-time Interactive Long Video Generation
            </h1>
            """
        )
    gradio.Markdown(
        """
        <div style="text-align: center;">
            <strong style="font-size: 25px;">LongLive-1.3B</strong><br><br>
            <div style="display: flex; justify-content: center; align-items: center; gap: 20px;">
                <a href='https://github.com/NVlabs/LongLive'>
                    <img src='https://img.shields.io/badge/GitHub-LongLive-blue' alt='GitHub'>
                </a>
                <a href='https://huggingface.co/Efficient-Large-Model/LongLive-1.3B'>
                    <img src='https://img.shields.io/badge/HF%20Model-LongLive-bron' alt='GitHub'>
                </a>
                <a href='https://arxiv.org/abs/2509.22622'>
                    <img src='https://img.shields.io/badge/ArXiv-Paper-red' alt='ArXiv'>
                </a>
                <a href='https://www.youtube.com/watch?v=CO1QC7BNvig'>
                    <img src='https://img.shields.io/badge/YouTube-Intro-yellow' alt='YouTube'>
                </a>
            </div>
        </div>
        <strong> </strong>
        <p>
            We present LongLive, a frame-level autoregressive (AR) framework for real-time and interactive long video generation. Long video generation presents challenges in both efficiency and quality. Diffusion and Diffusion-Forcing models can produce high-quality videos but suffer from low efficiency due to bidirectional attention. Causal attention AR models support KV caching for faster inference, but often degrade in quality on long videos due to memory challenges during long-video training. In addition, beyond static prompt-based generation, interactive capabilities, such as streaming prompt inputs, are critical for dynamic content creation, enabling users to guide narratives in real time. This interactive requirement significantly increases complexity, especially in ensuring visual consistency and semantic coherence during prompt transitions. To address these challenges, LongLive adopts a causal, frame-level AR design that integrates a KV-recache mechanism that refreshes cached states with new prompts for smooth, adherent switches; streaming long tuning to enable long video training and to align training and inference (train-long-test-long); and short window attention paired with a frame-level attention sink, shorten as frame sink, preserving long-range consistency while enabling faster generation. With these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model to minute-long generation in just 32 GPU-days. At inference, LongLive sustains 20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both short and long videos. LongLive supports up to 240-second videos on a single H100 GPU. LongLive further supports INT8-quantized inference with only marginal quality loss.
        </p>
        """
    )

    gradio.Markdown("# Prompt â†’ Video ç¤ºä¾‹\nä¸Šæ–¹è¾“å…¥ promptï¼Œç‚¹å‡» Generateï¼Œä¸‹æ–¹ä¼šæ˜¾ç¤ºç”Ÿæˆçš„è§†é¢‘ã€‚")

    with gradio.Column():
        seed = gradio.Textbox(label="Seed (-1 for random)", value="42")
        p1 = gradio.Textbox(label="Prompt", lines=2)
        generate_btn = gradio.Button("Generate")
        download_btn = gradio.Button("ä¸‹è½½å†å²æ‹¼æ¥è§†é¢‘")

    # æˆç‰‡ï¼šç­‰æœ€ç»ˆå†™å®Œ mp4 åå±•ç¤º
    video_output = gradio.Video(label="æ–°ç”Ÿæˆçš„è§†é¢‘", autoplay=True, elem_id="final_video")
    concat_file = gradio.File(label="å†å²æ‹¼æ¥è§†é¢‘ï¼ˆç‚¹å‡»ä¸‹è½½ï¼‰")
    history_state = gradio.State([])

    # ç”Ÿæˆå•æ®µè§†é¢‘ï¼ŒåŒæ—¶æ›´æ–°å½“å‰ä¼šè¯çš„å†å²åˆ—è¡¨
    generate_btn.click(
        fn=on_generate_stream,
        inputs=[p1, seed, history_state],
        outputs=[video_output, history_state],
    )

    # ä¸‹è½½ï¼šå°†åŒä¸€ä¼šè¯ä¸­çš„å†å²è§†é¢‘æŒ‰é¡ºåºæ‹¼æ¥ï¼ˆæœ€å¤š 6 æ®µï¼‰
    download_btn.click(
        fn=concat_history_videos,
        inputs=[history_state],
        outputs=[concat_file],
    )

if __name__ == '__main__':
    demo.queue(max_size=16, default_concurrency_limit=1)
    demo.launch(share=True)
